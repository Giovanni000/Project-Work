{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STEP 1: Classificatore OCCLUSION vs VISIBLE\n",
        "\n",
        "Classificatore binario per distinguere immagini occluse da immagini visibili.\n",
        "\n",
        "**Label mapping:**\n",
        "- `OCCLUSION` / `PARTIAL OCCLUSION` → classe 0\n",
        "- `OK` / `KO` → classe 1 (VISIBLE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Clona repository GitHub e monta Google Drive per i dati\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Opzione 1: Clona da GitHub (consigliato per sviluppo)\n",
        "# Sostituisci con il tuo repository URL\n",
        "GITHUB_REPO = \"https://github.com/Giovanni000/Project-Work.git\"  # ⚠️ MODIFICA QUESTO!\n",
        "REPO_DIR = \"/content/project\"\n",
        "\n",
        "# Clona repository (se non esiste già)\n",
        "if not Path(REPO_DIR).exists():\n",
        "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
        "\n",
        "# Cambia directory al repository\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"Repository directory: {os.getcwd()}\")\n",
        "\n",
        "# Opzione 2: Monta Google Drive solo per i dati (immagini)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ai dati su Drive\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Project Work/Data\")\n",
        "print(f\"Data directory: {DATA_ROOT}\")\n",
        "\n",
        "# Crea symlink o copia i dati se necessario\n",
        "# Oppure modifica i path nel CSV per puntare a Drive\n",
        "\n",
        "# Import necessari\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Seed per riproducibilità\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Verifica device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OcclusionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset PyTorch per classificazione OCCLUSION vs VISIBLE.\n",
        "    \n",
        "    Label mapping:\n",
        "    - OCCLUSION (o PARTIAL OCCLUSION) → 0\n",
        "    - OK o KO → 1 (VISIBLE)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_path: Path al CSV con colonne 'image_path' e 'label'\n",
        "            transform: Trasformazioni da applicare alle immagini\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Mappa label: OCCLUSION → 0, OK/KO → 1\n",
        "        self.label_map = {\n",
        "            'OCCLUSION': 0,\n",
        "            'OK': 1,\n",
        "            'KO': 1\n",
        "        }\n",
        "        \n",
        "        print(f\"Dataset caricato: {len(self.df)} immagini\")\n",
        "        print(f\"Distribuzione label originale:\")\n",
        "        print(self.df['label'].value_counts())\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        label_str = row['label']\n",
        "        \n",
        "        # Carica immagine\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        # Applica trasformazioni\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Converti label\n",
        "        label = self.label_map.get(label_str, 1)  # Default a VISIBLE se label sconosciuta\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modello CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OcclusionCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN semplice per classificazione binaria OCCLUSION vs VISIBLE.\n",
        "    \n",
        "    Architettura:\n",
        "    - 3 layer convoluzionali con pooling\n",
        "    - 2 layer fully connected\n",
        "    - Output: 2 classi (OCCLUSION, VISIBLE)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(OcclusionCNN, self).__init__()\n",
        "        \n",
        "        # Encoder convoluzionale\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 128x128 -> 64x64\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 64x64 -> 32x32\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
        "        \n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)  # 2 classi: OCCLUSION, VISIBLE\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convoluzioni\n",
        "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully connected\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_occlusion_classifier(csv_path=\"data/dataset.csv\", \n",
        "                               val_fraction=0.2,\n",
        "                               batch_size=32,\n",
        "                               num_epochs=20,\n",
        "                               learning_rate=0.001,\n",
        "                               device=None):\n",
        "    \"\"\"\n",
        "    Training loop per il classificatore OCCLUSION vs VISIBLE.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    \n",
        "    # Trasformazioni per le immagini\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "    ])\n",
        "    \n",
        "    # Crea dataset\n",
        "    full_dataset = OcclusionDataset(csv_path, transform=transform)\n",
        "    \n",
        "    # Split train/val\n",
        "    total_size = len(full_dataset)\n",
        "    val_size = int(val_fraction * total_size)\n",
        "    train_size = total_size - val_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset, \n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nSplit dataset:\")\n",
        "    print(f\"  Train: {len(train_dataset)} immagini\")\n",
        "    print(f\"  Val: {len(val_dataset)} immagini\")\n",
        "    \n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    # Modello\n",
        "    model = OcclusionCNN().to(device)\n",
        "    print(f\"\\nModello creato. Parametri totali: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # Loss e optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # Training loop\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(f\"\\nInizio training per {num_epochs} epoch...\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        # Stampa risultati\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        \n",
        "        # Salva miglior modello\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"  ✓ Nuovo miglior modello! Val Acc: {best_val_acc:.2f}%\")\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    # Carica miglior modello\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"\\n✅ Training completato!\")\n",
        "    print(f\"   Miglior Val Accuracy: {best_val_acc:.2f}%\")\n",
        "    \n",
        "    # Salva modello\n",
        "    models_dir = Path(\"models\")\n",
        "    models_dir.mkdir(exist_ok=True)\n",
        "    model_path = models_dir / \"occlusion_cnn.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"   Modello salvato in: {model_path}\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training del classificatore\n",
        "model_occ = train_occlusion_classifier(\n",
        "    csv_path=\"data/dataset.csv\",\n",
        "    val_fraction=0.2,\n",
        "    batch_size=32,\n",
        "    num_epochs=20,\n",
        "    learning_rate=0.001,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funzione Helper per Caricare il Modello\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_occlusion_model(device=None):\n",
        "    \"\"\"\n",
        "    Carica il modello addestrato per classificazione OCCLUSION vs VISIBLE.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = OcclusionCNN().to(device)\n",
        "    model_path = Path(\"models/occlusion_cnn.pth\")\n",
        "    \n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
        "    \n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
