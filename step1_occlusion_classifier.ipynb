{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STEP 1: Classificatore OCCLUSION vs VISIBLE\n",
        "\n",
        "Classificatore binario per distinguere immagini occluse da immagini visibili.\n",
        "\n",
        "**Label mapping:**\n",
        "- `OCCLUSION` / `PARTIAL OCCLUSION` ‚Üí classe 0\n",
        "- `OK` / `KO` ‚Üí classe 1 (VISIBLE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Clona repository GitHub e monta Google Drive per i dati\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Opzione 1: Clona da GitHub (consigliato per sviluppo)\n",
        "# Sostituisci con il tuo repository URL\n",
        "GITHUB_REPO = \"https://github.com/Giovanni000/Project-Work.git\"  # ‚ö†Ô∏è MODIFICA QUESTO!\n",
        "REPO_DIR = \"/content/project\"\n",
        "\n",
        "# Clona repository (se non esiste gi√†)\n",
        "if not Path(REPO_DIR).exists():\n",
        "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
        "\n",
        "# Cambia directory al repository\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"Repository directory: {os.getcwd()}\")\n",
        "\n",
        "# Opzione 2: Monta Google Drive solo per i dati (immagini)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ai dati su Drive\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Project Work/Data\")\n",
        "print(f\"Data directory: {DATA_ROOT}\")\n",
        "\n",
        "# ‚ö†Ô∏è IMPORTANTE: Le immagini su Drive sono LENTE da caricare durante il training!\n",
        "# Se il training √® troppo lento, considera di copiare le immagini in locale prima\n",
        "# (vedi cella opzionale sotto)\n",
        "\n",
        "# Import necessari\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Seed per riproducibilit√†\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Verifica device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"‚úÖ Tesla T4 rilevata - Parametri ottimizzati per questa GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OcclusionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset PyTorch per classificazione OCCLUSION vs VISIBLE.\n",
        "    \n",
        "    Label mapping:\n",
        "    - OCCLUSION (o PARTIAL OCCLUSION) ‚Üí 0\n",
        "    - OK o KO ‚Üí 1 (VISIBLE)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_path: Path al CSV con colonne 'image_path' e 'label'\n",
        "            transform: Trasformazioni da applicare alle immagini\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Mappa label: OCCLUSION ‚Üí 0, OK/KO ‚Üí 1\n",
        "        self.label_map = {\n",
        "            'OCCLUSION': 0,\n",
        "            'OK': 1,\n",
        "            'KO': 1\n",
        "        }\n",
        "        \n",
        "        print(f\"Dataset caricato: {len(self.df)} immagini\")\n",
        "        print(f\"Distribuzione label originale:\")\n",
        "        print(self.df['label'].value_counts())\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        label_str = row['label']\n",
        "        \n",
        "        # Carica immagine (ottimizzato: evita lazy loading)\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            # Forza il caricamento completo dell'immagine\n",
        "            image.load()\n",
        "        except Exception as e:\n",
        "            print(f\"Errore caricamento {image_path}: {e}\")\n",
        "            # Fallback: immagine nera\n",
        "            image = Image.new('RGB', (128, 128), (0, 0, 0))\n",
        "        \n",
        "        # Applica trasformazioni\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Converti label\n",
        "        label = self.label_map.get(label_str, 1)  # Default a VISIBLE se label sconosciuta\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modello CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OcclusionCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN semplice per classificazione binaria OCCLUSION vs VISIBLE.\n",
        "    \n",
        "    Architettura:\n",
        "    - 3 layer convoluzionali con pooling\n",
        "    - 2 layer fully connected\n",
        "    - Output: 2 classi (OCCLUSION, VISIBLE)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(OcclusionCNN, self).__init__()\n",
        "        \n",
        "        # Encoder convoluzionale\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 128x128 -> 64x64\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 64x64 -> 32x32\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
        "        \n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)  # 2 classi: OCCLUSION, VISIBLE\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convoluzioni\n",
        "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully connected\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_occlusion_classifier(csv_path=\"data/dataset.csv\", \n",
        "                               val_fraction=0.2,\n",
        "                               batch_size=32,\n",
        "                               num_epochs=20,\n",
        "                               learning_rate=0.001,\n",
        "                               device=None):\n",
        "    \"\"\"\n",
        "    Training loop per il classificatore OCCLUSION vs VISIBLE.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    \n",
        "    # Trasformazioni per le immagini\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "    ])\n",
        "    \n",
        "    # Crea dataset\n",
        "    full_dataset = OcclusionDataset(csv_path, transform=transform)\n",
        "    \n",
        "    # Split train/val\n",
        "    total_size = len(full_dataset)\n",
        "    val_size = int(val_fraction * total_size)\n",
        "    train_size = total_size - val_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset, \n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nSplit dataset:\")\n",
        "    print(f\"  Train: {len(train_dataset)} immagini\")\n",
        "    print(f\"  Val: {len(val_dataset)} immagini\")\n",
        "    \n",
        "    # DataLoaders (ottimizzati per Tesla T4)\n",
        "    # num_workers=2: buon compromesso per T4 (puoi provare 4 se hai CPU veloce)\n",
        "    # pin_memory=True: essenziale per velocizzare CPU->GPU su T4\n",
        "    # prefetch_factor=2: pre-carica batch per ridurre tempi di attesa\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=2,  # 2-4 ottimale per T4\n",
        "        pin_memory=True if device.type == 'cuda' else False,\n",
        "        persistent_workers=False,\n",
        "        prefetch_factor=2  # Pre-carica 2 batch\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=False, \n",
        "        num_workers=2,  # 2-4 ottimale per T4\n",
        "        pin_memory=True if device.type == 'cuda' else False,\n",
        "        persistent_workers=False,\n",
        "        prefetch_factor=2  # Pre-carica 2 batch\n",
        "    )\n",
        "    \n",
        "    # Modello\n",
        "    model = OcclusionCNN().to(device)\n",
        "    print(f\"\\nModello creato. Parametri totali: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # Loss e optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # Training loop\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(f\"\\nInizio training per {num_epochs} epoch...\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        \n",
        "        # Stampa risultati\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        \n",
        "        # Salva miglior modello\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"  ‚úì Nuovo miglior modello! Val Acc: {best_val_acc:.2f}%\")\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    # Carica miglior modello\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"\\n‚úÖ Training completato!\")\n",
        "    print(f\"   Miglior Val Accuracy: {best_val_acc:.2f}%\")\n",
        "    \n",
        "    # Salva modello\n",
        "    models_dir = Path(\"models\")\n",
        "    models_dir.mkdir(exist_ok=True)\n",
        "    model_path = models_dir / \"occlusion_cnn.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"   Modello salvato in: {model_path}\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Ottimizzazione: Copia Immagini in Locale (OPZIONALE)\n",
        "\n",
        "**Se il training √® troppo lento**, copia le immagini da Drive in locale prima del training.\n",
        "Questo accelera drasticamente il caricamento durante il training.\n",
        "\n",
        "**Nota:** Richiede spazio su disco (~500MB-1GB per ~2853 immagini 128x128).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö° IMPORTANTE: Copia immagini in locale per velocizzare il training\n",
        "# Il training √® lento perch√© legge ogni immagine da Google Drive (latenza alta)\n",
        "# Copiando in locale, il training diventa 10-20x pi√π veloce!\n",
        "\n",
        "COPY_TO_LOCAL = True  # Cambia a False se vuoi usare Drive direttamente (LENTO!)\n",
        "\n",
        "if COPY_TO_LOCAL:\n",
        "    import shutil\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    LOCAL_DATA_DIR = Path(\"/content/local_data\")\n",
        "    LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Leggi CSV per ottenere tutti i path\n",
        "    csv_path = Path(\"data/dataset.csv\")\n",
        "    if csv_path.exists():\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"üì¶ Copiando {len(df)} immagini da Drive in locale...\")\n",
        "        print(\"   Questo richiede ~2-5 minuti, ma accelera il training di 10-20x!\")\n",
        "        print(\"   \" + \"=\"*60)\n",
        "        \n",
        "        copied = 0\n",
        "        skipped = 0\n",
        "        errors = 0\n",
        "        \n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Copia immagini\"):\n",
        "            src_path = Path(row['image_path'])\n",
        "            # Mantieni struttura: connector_name/filename\n",
        "            rel_path = Path(row['connector_name']) / row['filename']\n",
        "            dst_path = LOCAL_DATA_DIR / rel_path\n",
        "            \n",
        "            if dst_path.exists():\n",
        "                skipped += 1\n",
        "            else:\n",
        "                dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                try:\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "                    copied += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Errore copia {src_path}: {e}\")\n",
        "                    errors += 1\n",
        "        \n",
        "        print(\"   \" + \"=\"*60)\n",
        "        print(f\"‚úÖ Copiate {copied} immagini nuove\")\n",
        "        print(f\"‚è≠Ô∏è  Saltate {skipped} immagini gi√† presenti\")\n",
        "        if errors > 0:\n",
        "            print(f\"‚ùå Errori: {errors}\")\n",
        "        \n",
        "        # Aggiorna i path nel CSV per puntare a locale\n",
        "        print(\"\\nüîÑ Aggiornamento path nel CSV...\")\n",
        "        df['image_path'] = df.apply(\n",
        "            lambda row: str(LOCAL_DATA_DIR / row['connector_name'] / row['filename']),\n",
        "            axis=1\n",
        "        )\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"‚úÖ CSV aggiornato: i path ora puntano a {LOCAL_DATA_DIR}\")\n",
        "        print(f\"\\nüöÄ Ora il training sar√† MOLTO pi√π veloce!\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  data/dataset.csv non trovato. Esegui prima la preparazione del dataset.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Copia locale disabilitata.\")\n",
        "    print(\"‚ö†Ô∏è  ATTENZIONE: Il training sar√† LENTO (10-20x pi√π lento) perch√© legge da Drive!\")\n",
        "    print(\"   Imposta COPY_TO_LOCAL = True per velocizzare.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training del classificatore\n",
        "model_occ = train_occlusion_classifier(\n",
        "    csv_path=\"data/dataset.csv\",\n",
        "    val_fraction=0.2,\n",
        "    batch_size=32,\n",
        "    num_epochs=20,\n",
        "    learning_rate=0.001,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funzione Helper per Caricare il Modello\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_occlusion_model(device=None):\n",
        "    \"\"\"\n",
        "    Carica il modello addestrato per classificazione OCCLUSION vs VISIBLE.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = OcclusionCNN().to(device)\n",
        "    model_path = Path(\"models/occlusion_cnn.pth\")\n",
        "    \n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
        "    \n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
