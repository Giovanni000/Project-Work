{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Autoencoder per Anomaly Detection - UN MODELLO PER OGNI CONNETTORE\n",
    "\n",
    "Questo notebook addestra **9 modelli separati**, uno per ogni connettore (conn1, conn2, ..., conn9).\n",
    "\n",
    "Ogni modello avrà il suo threshold specifico calcolato solo sui dati OK del rispettivo connettore.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Clona repository GitHub e monta Google Drive per i dati\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Opzione 1: Clona da GitHub (consigliato per sviluppo)\n",
    "# Sostituisci con il tuo repository URL\n",
    "GITHUB_REPO = \"https://github.com/Giovanni000/Project-Work.git\"  # ⚠️ MODIFICA QUESTO!\n",
    "REPO_DIR = \"/content/project\"\n",
    "\n",
    "# Clona repository (se non esiste già)\n",
    "if not Path(REPO_DIR).exists():\n",
    "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
    "else:\n",
    "    os.chdir(REPO_DIR)\n",
    "    !git pull\n",
    "\n",
    "# Cambia directory al repository\n",
    "os.chdir(REPO_DIR)\n",
    "# Se il clone crea una sottocartella, entra dentro\n",
    "subdirs = [d for d in Path(REPO_DIR).iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "if len(subdirs) == 1:\n",
    "    os.chdir(subdirs[0])\n",
    "\n",
    "print(f\"Repository directory: {os.getcwd()}\")\n",
    "\n",
    "# Opzione 2: Monta Google Drive solo per i dati (immagini)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path ai dati su Drive\n",
    "DATA_ROOT = Path(\"/content/drive/MyDrive/Project Work/Data\")\n",
    "print(f\"Data directory: {DATA_ROOT}\")\n",
    "\n",
    "# ⚠️ IMPORTANTE: Le immagini su Drive sono LENTE da caricare durante il training!\n",
    "# Se il training è troppo lento, considera di copiare le immagini in locale prima\n",
    "\n",
    "# Import necessari\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Seed per riproducibilità\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Verifica device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    if \"T4\" in gpu_name:\n",
    "        print(\"✅ Tesla T4 rilevata - Parametri ottimizzati per questa GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class (solo OK, filtrato per connettore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEDatasetPerConnector(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch per autoencoder di un singolo connettore.\n",
    "    Contiene solo immagini OK del connettore specificato.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, connector_name, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path: Path al CSV con colonne 'image_path', 'label', 'connector_name'\n",
    "            connector_name: Nome del connettore (es. 'conn1', 'conn2', ...)\n",
    "            transform: Trasformazioni da applicare alle immagini\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Filtra solo OK del connettore specificato\n",
    "        self.df = df[(df['label'] == 'OK') & (df['connector_name'] == connector_name)].copy().reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"Dataset AE per {connector_name}: {len(self.df)} immagini OK\")\n",
    "        if len(self.df) == 0:\n",
    "            raise ValueError(f\"Nessuna immagine OK trovata per {connector_name}!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        \n",
    "        # Carica immagine (ottimizzato: evita lazy loading)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image.load()  # Forza caricamento completo\n",
    "        except Exception as e:\n",
    "            print(f\"Errore caricamento {image_path}: {e}\")\n",
    "            image = Image.new('RGB', (128, 128), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modello Autoencoder Convoluzionale (stesso di prima)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder convoluzionale per anomaly detection.\n",
    "    \n",
    "    Encoder: 3 conv2d con stride=2 (3→16→32→64 canali)\n",
    "    Decoder: 3 convtranspose2d simmetriche (64→32→16→3 canali)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 128x128x3 -> 64x64x16\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 64x64x16 -> 32x32x32\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 32x32x32 -> 16x16x64\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 16x16x64 -> 32x32x32\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 32x32x32 -> 64x64x16\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 64x64x16 -> 128x128x3\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione Training (per un singolo connettore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_per_connector(connector_name, csv_path=\"data/dataset.csv\",\n",
    "                                     batch_size=128,\n",
    "                                     num_epochs=30,\n",
    "                                     learning_rate=0.001,\n",
    "                                     device=None):\n",
    "    \"\"\"\n",
    "    Addestra un autoencoder per un singolo connettore.\n",
    "    \n",
    "    Args:\n",
    "        connector_name: Nome del connettore (es. 'conn1')\n",
    "        csv_path: Path al CSV del dataset\n",
    "        batch_size: Dimensione del batch\n",
    "        num_epochs: Numero di epoche\n",
    "        learning_rate: Learning rate\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        model: Modello addestrato\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training autoencoder per {connector_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Trasformazioni\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Dataset (solo OK del connettore specificato)\n",
    "    dataset = AEDatasetPerConnector(csv_path, connector_name, transform=transform)\n",
    "    \n",
    "    # DataLoader (ottimizzato per Colab)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,  # 2 per Tesla T4\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    # Modello\n",
    "    model = ConvAE().to(device)\n",
    "    \n",
    "    # Loss e Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Salva modello\n",
    "    models_dir = Path(\"models\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    model_path = models_dir / f\"ae_conv_{connector_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\n✅ Modello salvato in: {model_path}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione Calcolo Threshold (per un singolo connettore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold_per_connector(model, connector_name, csv_path=\"data/dataset.csv\", device=None):\n",
    "    \"\"\"\n",
    "    Calcola il threshold per anomaly detection per un singolo connettore.\n",
    "    \n",
    "    Threshold = mu + 3*sigma, dove mu e sigma sono media e std degli errori\n",
    "    di ricostruzione su tutte le immagini OK del connettore specificato.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"\\nCalcolo threshold per {connector_name}...\")\n",
    "    \n",
    "    # Trasformazioni\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Dataset (solo OK del connettore specificato)\n",
    "    dataset = AEDatasetPerConnector(csv_path, connector_name, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    errors = []\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(loader, desc=\"Calcolo errori\"):\n",
    "            images = images.to(device)\n",
    "            reconstructed = model(images)\n",
    "            \n",
    "            batch_errors = criterion(reconstructed, images)\n",
    "            batch_errors = batch_errors.mean(dim=(1, 2, 3))\n",
    "            \n",
    "            errors.extend(batch_errors.cpu().numpy())\n",
    "    \n",
    "    errors = np.array(errors)\n",
    "    mu = np.mean(errors)\n",
    "    sigma = np.std(errors)\n",
    "    \n",
    "    # ⚙️ SCEGLI IL METODO PER IL THRESHOLD:\n",
    "    # Opzione 1: mu + 3*sigma (più conservativo, meno falsi positivi, ma più falsi negativi)\n",
    "    # Opzione 2: mu + 2*sigma (più sensibile, rileva più KO, ma più falsi positivi)\n",
    "    # Opzione 3: mu + 2.5*sigma (compromesso)\n",
    "    \n",
    "    # Se non rilevi KO, prova a ridurre il moltiplicatore (es. 2 o 2.5 invece di 3)\n",
    "    SIGMA_MULTIPLIER = 2.5  # ⚠️ MODIFICA QUESTO se non rilevi KO (prova 2.0 o 2.5)\n",
    "    threshold = mu + SIGMA_MULTIPLIER * sigma\n",
    "    \n",
    "    print(f\"\\nStatistiche errori per {connector_name}:\")\n",
    "    print(f\"  Media (mu): {mu:.6f}\")\n",
    "    print(f\"  Std (sigma): {sigma:.6f}\")\n",
    "    print(f\"  Min: {errors.min():.6f}\")\n",
    "    print(f\"  Max: {errors.max():.6f}\")\n",
    "    print(f\"\\nThreshold (mu + 3*sigma): {threshold:.6f}\")\n",
    "    \n",
    "    # Salva threshold\n",
    "    models_dir = Path(\"models\")\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    threshold_path = models_dir / f\"ae_threshold_{connector_name}.npy\"\n",
    "    np.save(threshold_path, threshold)\n",
    "    print(f\"  Threshold salvato in: {threshold_path}\")\n",
    "    \n",
    "    return threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestra Tutti i 9 Modelli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei 9 connettori\n",
    "connectors = [f\"conn{i}\" for i in range(1, 10)]\n",
    "\n",
    "# Verifica che il CSV esista\n",
    "csv_path = Path(\"data/dataset.csv\")\n",
    "if not csv_path.exists():\n",
    "    print(\"⚠️  data/dataset.csv non trovato! Esegui prima training_pipeline.ipynb\")\n",
    "    raise FileNotFoundError(\"data/dataset.csv non trovato\")\n",
    "\n",
    "# ⚙️ CONFIGURAZIONE TRAINING\n",
    "# Se la loss sta ancora scendendo, aumenta le epoche\n",
    "# Nota: Se la loss scende molto lentamente (< 0.00002 per epoca), i guadagni sono minimi\n",
    "# Per anomaly detection, una loss < 0.002 è già eccellente\n",
    "NUM_EPOCHS = 100  # Aumentato a 100 (puoi aumentare a 120-150 se la loss scende ancora lentamente)\n",
    "BATCH_SIZE = 128  # Ottimizzato per Tesla T4\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"⚙️  Configurazione:\")\n",
    "print(f\"   Epoche: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Addestra un modello per ogni connettore\n",
    "models = {}\n",
    "thresholds = {}\n",
    "\n",
    "for connector_name in connectors:\n",
    "    try:\n",
    "        # Training\n",
    "        model = train_autoencoder_per_connector(\n",
    "            connector_name=connector_name,\n",
    "            csv_path=str(csv_path),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            device=device\n",
    "        )\n",
    "        models[connector_name] = model\n",
    "        \n",
    "        # Calcola threshold\n",
    "        threshold = calculate_threshold_per_connector(\n",
    "            model=model,\n",
    "            connector_name=connector_name,\n",
    "            csv_path=str(csv_path),\n",
    "            device=device\n",
    "        )\n",
    "        thresholds[connector_name] = threshold\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore per {connector_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ Training completato per {len(models)} connettori\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nRiepilogo thresholds:\")\n",
    "for conn, thresh in thresholds.items():\n",
    "    print(f\"  {conn}: {thresh:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione Helper per Caricare Modello e Threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ae_and_threshold_per_connector(connector_name, device=None):\n",
    "    \"\"\"\n",
    "    Carica l'autoencoder addestrato e il threshold per un singolo connettore.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Carica modello\n",
    "    model = ConvAE().to(device)\n",
    "    model_path = Path(f\"models/ae_conv_{connector_name}.pth\")\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # Carica threshold\n",
    "    threshold_path = Path(f\"models/ae_threshold_{connector_name}.npy\")\n",
    "    if not threshold_path.exists():\n",
    "        raise FileNotFoundError(f\"Threshold non trovato: {threshold_path}\")\n",
    "    \n",
    "    threshold = np.load(threshold_path)\n",
    "    \n",
    "    return model, threshold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
