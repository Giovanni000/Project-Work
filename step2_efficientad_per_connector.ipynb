{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STEP 2: EfficientAD-M per Anomaly Detection - UN MODELLO PER OGNI CONNETTORE\n",
        "\n",
        "Questo notebook addestra **9 modelli EfficientAD-M separati**, uno per ogni connettore (conn1, conn2, ..., conn9).\n",
        "\n",
        "Ogni modello avrà il suo threshold specifico calcolato solo sui dati OK del rispettivo connettore.\n",
        "\n",
        "**EfficientAD-M** è un metodo di anomaly detection basato su Teacher-Student architecture:\n",
        "- **Teacher**: ResNet18 pre-addestrato su ImageNet (congelato)\n",
        "- **Student**: ResNet18 non pre-addestrato (addestrato su OK)\n",
        "- **Anomaly score**: differenza tra feature teacher e student\n",
        "\n",
        "Funziona meglio degli autoencoder quando le anomalie sono strutturali/geometriche (come nel nostro caso).\n",
        "\n",
        "**NOTA**: Le immagini sono già in grayscale e normalizzate nel preprocessing. Le carichiamo come RGB per compatibilità con ResNet pre-addestrato.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Clona repository GitHub e monta Google Drive per i dati\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Opzione 1: Clona da GitHub (consigliato per sviluppo)\n",
        "# Sostituisci con il tuo repository URL\n",
        "GITHUB_REPO = \"https://github.com/Giovanni000/Project-Work.git\"  # ⚠️ MODIFICA QUESTO!\n",
        "REPO_DIR = \"/content/project\"\n",
        "\n",
        "# Clona repository (se non esiste già)\n",
        "if not Path(REPO_DIR).exists():\n",
        "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
        "else:\n",
        "    os.chdir(REPO_DIR)\n",
        "    !git pull\n",
        "\n",
        "# Cambia directory al repository\n",
        "os.chdir(REPO_DIR)\n",
        "# Se il clone crea una sottocartella, entra dentro\n",
        "subdirs = [d for d in Path(REPO_DIR).iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
        "if len(subdirs) == 1:\n",
        "    os.chdir(subdirs[0])\n",
        "\n",
        "print(f\"Repository directory: {os.getcwd()}\")\n",
        "\n",
        "# Opzione 2: Monta Google Drive solo per i dati (immagini)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ai dati su Drive\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/Project Work/Data\")\n",
        "print(f\"Data directory: {DATA_ROOT}\")\n",
        "\n",
        "# ⚠️ IMPORTANTE: Le immagini su Drive sono LENTE da caricare durante il training!\n",
        "# Se il training è troppo lento, considera di copiare le immagini in locale prima\n",
        "\n",
        "# Import necessari\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Seed per riproducibilità\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Verifica device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"✅ Tesla T4 rilevata - Parametri ottimizzati per questa GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verifica/Crea Dataset CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se data/dataset.csv non esiste, crealo automaticamente\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_csv = Path(\"data/dataset.csv\")\n",
        "\n",
        "if not dataset_csv.exists():\n",
        "    print(\"⚠️  data/dataset.csv non trovato. Creazione automatica...\")\n",
        "    \n",
        "    # Carica features_labeled.csv\n",
        "    features_csv = Path(\"features_labeled.csv\")\n",
        "    if not features_csv.exists():\n",
        "        # Prova path alternativo\n",
        "        features_csv = Path(\"/content/project/features_labeled.csv\")\n",
        "    \n",
        "    if not features_csv.exists():\n",
        "        raise FileNotFoundError(f\"features_labeled.csv non trovato in {features_csv}\")\n",
        "    \n",
        "    print(f\"Leggendo CSV: {features_csv}...\")\n",
        "    df = pd.read_csv(features_csv)\n",
        "    print(f\"CSV caricato: {len(df)} righe\")\n",
        "    \n",
        "    # Aggrega PARTIAL OCCLUSION con OCCLUSION\n",
        "    df['label_merged'] = df['label'].replace('PARTIAL OCCLUSION', 'OCCLUSION')\n",
        "    \n",
        "    # Costruisci path immagini (su Drive)\n",
        "    DRIVE_DATA_BASE = \"/content/drive/MyDrive/Project Work/Data\"\n",
        "    df['image_path'] = df.apply(\n",
        "        lambda row: f\"{DRIVE_DATA_BASE}/connectors/{row['connector_name']}/{row['filename']}\",\n",
        "        axis=1\n",
        "    )\n",
        "    \n",
        "    # Verifica esistenza immagini\n",
        "    print(\"Verificando esistenza immagini...\")\n",
        "    existing = []\n",
        "    for idx, path in enumerate(df['image_path']):\n",
        "        if Path(path).exists():\n",
        "            existing.append(idx)\n",
        "    \n",
        "    print(f\"Immagini trovate: {len(existing)}/{len(df)}\")\n",
        "    \n",
        "    if len(existing) == 0:\n",
        "        print(\"⚠️  PROBLEMA: Nessuna immagine trovata!\")\n",
        "        print(\"Verifica che Drive sia montato e che i path siano corretti.\")\n",
        "    \n",
        "    # Filtra solo immagini esistenti\n",
        "    df_valid = df.iloc[existing].copy()\n",
        "    \n",
        "    # Prepara CSV finale\n",
        "    output_df = df_valid[['image_path', 'label_merged', 'connector_name']].copy()\n",
        "    output_df.rename(columns={'label_merged': 'label'}, inplace=True)\n",
        "    \n",
        "    # Crea cartella data se non esiste\n",
        "    dataset_csv.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Salva\n",
        "    output_df.to_csv(dataset_csv, index=False)\n",
        "    print(f\"✅ Dataset preparato: {len(output_df)} righe\")\n",
        "    print(f\"Distribuzione label:\")\n",
        "    print(output_df['label'].value_counts())\n",
        "else:\n",
        "    print(f\"✅ Dataset trovato: {dataset_csv}\")\n",
        "    df_check = pd.read_csv(dataset_csv)\n",
        "    print(f\"  Righe: {len(df_check)}\")\n",
        "    print(f\"  Colonne: {list(df_check.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurazione EfficientAD-M\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURAZIONE EFFICIENTAD-M\n",
        "# ============================================================================\n",
        "\n",
        "# Dimensioni immagine (coerente con il progetto: 128x128)\n",
        "IMG_SIZE = 128\n",
        "\n",
        "# Parametri training\n",
        "BATCH_SIZE = 32  # Ottimizzato per Tesla T4\n",
        "NUM_EPOCHS = 20  # EfficientAD-M converge velocemente\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Threshold\n",
        "THRESHOLD_MULTIPLIER = 2.5  # Threshold = mu + THRESHOLD_MULTIPLIER * sigma\n",
        "\n",
        "# DataLoader\n",
        "NUM_WORKERS = 2  # Ottimizzato per Tesla T4\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Configurazione EfficientAD-M:\")\n",
        "print(f\"  IMG_SIZE: {IMG_SIZE}\")\n",
        "print(f\"  BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"  NUM_EPOCHS: {NUM_EPOCHS}\")\n",
        "print(f\"  LEARNING_RATE: {LEARNING_RATE}\")\n",
        "print(f\"  THRESHOLD_MULTIPLIER: {THRESHOLD_MULTIPLIER}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class (solo OK, filtrato per connettore)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientADDatasetPerConnector(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset PyTorch per EfficientAD-M di un singolo connettore.\n",
        "    Contiene solo immagini OK del connettore specificato.\n",
        "    \n",
        "    NOTA: Le immagini sono già in grayscale e normalizzate nel preprocessing.\n",
        "    Le carichiamo come RGB per compatibilità con ResNet pre-addestrato.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, csv_path, connector_name, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_path: Path al CSV con colonne 'image_path', 'label', 'connector_name'\n",
        "            connector_name: Nome del connettore (es. 'conn1', 'conn2', ...)\n",
        "            transform: Trasformazioni da applicare alle immagini\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Filtra solo OK del connettore specificato\n",
        "        self.df = df[(df['label'] == 'OK') & (df['connector_name'] == connector_name)].copy().reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        \n",
        "        print(f\"Dataset EfficientAD per {connector_name}: {len(self.df)} immagini OK\")\n",
        "        if len(self.df) == 0:\n",
        "            raise ValueError(f\"Nessuna immagine OK trovata per {connector_name}!\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        \n",
        "        # Carica immagine (ottimizzato: evita lazy loading)\n",
        "        try:\n",
        "            # Le immagini sono già grayscale nel preprocessing, ma le carichiamo come RGB\n",
        "            # per compatibilità con ResNet pre-addestrato\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image.load()  # Forza caricamento completo\n",
        "        except Exception as e:\n",
        "            print(f\"Errore caricamento {image_path}: {e}\")\n",
        "            # Fallback: immagine nera\n",
        "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelli Teacher & Student\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Teacher(nn.Module):\n",
        "    \"\"\"\n",
        "    Teacher: ResNet18 pre-addestrato su ImageNet (congelato).\n",
        "    Usato per estrarre feature di riferimento.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Teacher, self).__init__()\n",
        "        # Carica ResNet18 pre-addestrato\n",
        "        base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        # Rimuovi layer finali (avgpool e fc), mantieni solo encoder\n",
        "        self.encoder = nn.Sequential(*list(base.children())[:-2])\n",
        "        \n",
        "        # Congela tutti i parametri (no training)\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor [B, 3, H, W]\n",
        "        Returns:\n",
        "            feature_map: Tensor [B, 512, Hf, Wf] dove Hf=H/32, Wf=W/32\n",
        "        \"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "class Student(nn.Module):\n",
        "    \"\"\"\n",
        "    Student: ResNet18 NON pre-addestrato (pesi random).\n",
        "    Addestrato per imitare le feature del Teacher su immagini OK.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Student, self).__init__()\n",
        "        # Carica ResNet18 SENZA pesi pre-addestrati\n",
        "        base = resnet18(weights=None)\n",
        "        # Rimuovi layer finali (avgpool e fc), mantieni solo encoder\n",
        "        self.encoder = nn.Sequential(*list(base.children())[:-2])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor [B, 3, H, W]\n",
        "        Returns:\n",
        "            feature_map: Tensor [B, 512, Hf, Wf] dove Hf=H/32, Wf=W/32\n",
        "        \"\"\"\n",
        "        return self.encoder(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funzione Training per Connettore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_efficientad_per_connector(connector_name, csv_path=\"data/dataset.csv\",\n",
        "                                     batch_size=32,\n",
        "                                     num_epochs=20,\n",
        "                                     learning_rate=1e-4,\n",
        "                                     device=None):\n",
        "    \"\"\"\n",
        "    Addestra un modello EfficientAD-M per un singolo connettore.\n",
        "    \n",
        "    Args:\n",
        "        connector_name: Nome del connettore (es. 'conn1')\n",
        "        csv_path: Path al CSV del dataset\n",
        "        batch_size: Dimensione del batch\n",
        "        num_epochs: Numero di epoche\n",
        "        learning_rate: Learning rate\n",
        "        device: Device (cuda/cpu)\n",
        "    \n",
        "    Returns:\n",
        "        teacher: Modello Teacher (congelato)\n",
        "        student: Modello Student addestrato\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training EfficientAD-M per {connector_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Trasformazioni (normalizzazione ImageNet per ResNet pre-addestrato)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Dataset (solo OK del connettore specificato)\n",
        "    dataset = EfficientADDatasetPerConnector(csv_path, connector_name, transform=transform)\n",
        "    \n",
        "    # DataLoader (ottimizzato per Colab)\n",
        "    train_loader = DataLoader(\n",
        "        dataset, \n",
        "        batch_size=batch_size, \n",
        "        shuffle=True, \n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True if device.type == 'cuda' else False,\n",
        "        prefetch_factor=2,\n",
        "        persistent_workers=False\n",
        "    )\n",
        "    \n",
        "    # Modelli\n",
        "    teacher = Teacher().to(device)\n",
        "    student = Student().to(device)\n",
        "    \n",
        "    # Teacher in modalità eval e congelato\n",
        "    teacher.eval()\n",
        "    for param in teacher.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Loss e Optimizer (solo per Student)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # Training loop\n",
        "    print(f\"\\nTraining Student per imitare Teacher su immagini OK...\")\n",
        "    student.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Feature Teacher (congelato, no grad)\n",
        "            with torch.no_grad():\n",
        "                teacher_features = teacher(images)\n",
        "            \n",
        "            # Feature Student (addestrato)\n",
        "            student_features = student(images)\n",
        "            \n",
        "            # Loss: differenza tra feature Teacher e Student\n",
        "            loss = criterion(student_features, teacher_features)\n",
        "            \n",
        "            # Backward solo su Student\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
        "    \n",
        "    # Salva modello Student\n",
        "    models_dir = Path(\"models\")\n",
        "    models_dir.mkdir(exist_ok=True)\n",
        "    model_path = models_dir / f\"efficientad_student_{connector_name}.pth\"\n",
        "    torch.save(student.state_dict(), model_path)\n",
        "    print(f\"\\n✅ Modello Student salvato in: {model_path}\")\n",
        "    \n",
        "    return teacher, student\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_threshold_per_connector(teacher, student, connector_name, csv_path=\"data/dataset.csv\", \n",
        "                                      threshold_multiplier=2.5, device=None):\n",
        "    \"\"\"\n",
        "    Calcola il threshold per anomaly detection per un singolo connettore.\n",
        "    \n",
        "    Threshold = mu + threshold_multiplier * sigma, dove mu e sigma sono media e std\n",
        "    degli anomaly score su tutte le immagini OK del connettore specificato.\n",
        "    \n",
        "    Anomaly score = differenza tra feature Teacher e Student (max su feature map).\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    print(f\"\\nCalcolo threshold per {connector_name}...\")\n",
        "    \n",
        "    # Trasformazioni (stesse del training)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Dataset (solo OK del connettore)\n",
        "    dataset = EfficientADDatasetPerConnector(csv_path, connector_name, transform=transform)\n",
        "    \n",
        "    # DataLoader\n",
        "    loader = DataLoader(\n",
        "        dataset, \n",
        "        batch_size=1,  # Batch size 1 per calcolo preciso\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Evita problemi con multiprocessing\n",
        "        pin_memory=False\n",
        "    )\n",
        "    \n",
        "    # Calcola score per tutte le immagini OK\n",
        "    scores = []\n",
        "    \n",
        "    teacher.eval()\n",
        "    student.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images in tqdm(loader, desc=f\"Calcolo score {connector_name}\"):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Feature Teacher e Student\n",
        "            t_feat = teacher(images)\n",
        "            s_feat = student(images)\n",
        "            \n",
        "            # Differenza feature (anomaly map)\n",
        "            diff = (t_feat - s_feat) ** 2\n",
        "            # Media sui canali: [B, C, H, W] -> [B, H, W]\n",
        "            amap = diff.mean(dim=1)\n",
        "            # Score immagine = max su tutta la feature map\n",
        "            score = amap.flatten(1).max(1)[0].cpu().item()\n",
        "            scores.append(score)\n",
        "    \n",
        "    scores = np.array(scores)\n",
        "    mu = np.mean(scores)\n",
        "    sigma = np.std(scores)\n",
        "    threshold = mu + threshold_multiplier * sigma\n",
        "    \n",
        "    print(f\"  Score OK - mean: {mu:.6f}, std: {sigma:.6f}\")\n",
        "    print(f\"  Threshold ({threshold_multiplier}*sigma): {threshold:.6f}\")\n",
        "    \n",
        "    # Salva threshold\n",
        "    models_dir = Path(\"models\")\n",
        "    threshold_path = models_dir / f\"efficientad_threshold_{connector_name}.npy\"\n",
        "    np.save(threshold_path, threshold)\n",
        "    print(f\"  ✅ Threshold salvato in: {threshold_path}\")\n",
        "    \n",
        "    return threshold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop - Tutti i Connettori\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training per tutti i 9 connettori\n",
        "connectors = [f\"conn{i}\" for i in range(1, 10)]\n",
        "\n",
        "trained_models = {}\n",
        "\n",
        "for connector_name in connectors:\n",
        "    try:\n",
        "        # Training\n",
        "        teacher, student = train_efficientad_per_connector(\n",
        "            connector_name=connector_name,\n",
        "            csv_path=\"data/dataset.csv\",\n",
        "            batch_size=BATCH_SIZE,\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            device=DEVICE\n",
        "        )\n",
        "        \n",
        "        trained_models[connector_name] = (teacher, student)\n",
        "        \n",
        "        # Calcolo threshold\n",
        "        threshold = calculate_threshold_per_connector(\n",
        "            teacher=teacher,\n",
        "            student=student,\n",
        "            connector_name=connector_name,\n",
        "            csv_path=\"data/dataset.csv\",\n",
        "            threshold_multiplier=THRESHOLD_MULTIPLIER,\n",
        "            device=DEVICE\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Errore durante training di {connector_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"✅ Training completato per {len(trained_models)} connettori\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funzione di Caricamento Modello\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_efficientad_model(connector_name, device=None):\n",
        "    \"\"\"\n",
        "    Carica un modello EfficientAD-M addestrato per un connettore.\n",
        "    \n",
        "    Args:\n",
        "        connector_name: Nome del connettore (es. 'conn1')\n",
        "        device: Device (cuda/cpu)\n",
        "    \n",
        "    Returns:\n",
        "        teacher: Modello Teacher\n",
        "        student: Modello Student\n",
        "        threshold: Threshold per anomaly detection\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    models_dir = Path(\"models\")\n",
        "    model_path = models_dir / f\"efficientad_student_{connector_name}.pth\"\n",
        "    threshold_path = models_dir / f\"efficientad_threshold_{connector_name}.npy\"\n",
        "    \n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
        "    \n",
        "    if not threshold_path.exists():\n",
        "        raise FileNotFoundError(f\"Threshold non trovato: {threshold_path}\")\n",
        "    \n",
        "    # Carica Teacher (sempre lo stesso, pre-addestrato)\n",
        "    teacher = Teacher().to(device)\n",
        "    teacher.eval()\n",
        "    \n",
        "    # Carica Student\n",
        "    student = Student().to(device)\n",
        "    student.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    student.eval()\n",
        "    \n",
        "    # Carica threshold\n",
        "    threshold = np.load(threshold_path)\n",
        "    \n",
        "    print(f\"✅ Modello EfficientAD-M caricato per {connector_name}\")\n",
        "    print(f\"  Threshold: {threshold:.6f}\")\n",
        "    \n",
        "    return teacher, student, threshold\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
