#!/usr/bin/env python3
"""Script per generare step4_visual_inference.ipynb completo."""

import json
from pathlib import Path

cells = []

# Cell 0: Markdown header
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "# STEP 4: Inferenza Visiva con Preprocessing Completo\n",
        "\n",
        "Questo notebook:\n",
        "1. Carica un'immagine PCB\n",
        "2. La allinea e normalizza (come `preprocess_alignment.py`)\n",
        "3. Estrae i 9 connettori (come `crop_connectors.py`)\n",
        "4. Classifica ogni connettore con il modello allenato\n",
        "5. Mostra i risultati con visualizzazione grafica"
    ]
})

# Cell 1: Setup
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Setup: Clona repository GitHub e monta Google Drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "GITHUB_REPO = \"https://github.com/Giovanni000/Project-Work.git\"  # ‚ö†Ô∏è MODIFICA QUESTO!\n",
        "REPO_DIR = \"/content/project\"\n",
        "\n",
        "if not Path(REPO_DIR).exists():\n",
        "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"Repository directory: {os.getcwd()}\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from dataclasses import dataclass\n",
        "import tempfile\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
    ]
})

# Cell 2: Markdown - Carica Modelli
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["## Carica Modelli (da step3)"]
})

# Cell 3: Definizione Classi Modelli (copio da step3)
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Definizione classe OcclusionCNN (da step1)\n",
        "class OcclusionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OcclusionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Definizione classe ConvAE (da step2)\n",
        "class ConvAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Funzioni per caricare i modelli\n",
        "def load_occlusion_model(device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = OcclusionCNN().to(device)\n",
        "    model_path = Path(\"models/occlusion_cnn.pth\")\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_ae_and_threshold(device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ConvAE().to(device)\n",
        "    model_path = Path(\"models/ae_conv.pth\")\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    threshold_path = Path(\"models/ae_threshold.npy\")\n",
        "    if not threshold_path.exists():\n",
        "        raise FileNotFoundError(f\"Threshold non trovato: {threshold_path}\")\n",
        "    threshold = np.load(threshold_path)\n",
        "    return model, threshold\n",
        "\n",
        "# Carica modelli\n",
        "print(\"Caricamento modelli...\")\n",
        "occ_model = load_occlusion_model(device)\n",
        "ae_model, threshold = load_ae_and_threshold(device)\n",
        "print(\"‚úÖ Modelli caricati!\")"
    ]
})

# Cell 4: Markdown - Funzioni Preprocessing
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["## Funzioni di Preprocessing e Classificazione"]
})

# Cell 5: Funzioni Preprocessing
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Funzioni di preprocessing (da step3)\n",
        "def preprocess_image(image_path, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image)\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "    return image_tensor\n",
        "\n",
        "def preprocess_image_for_ae(image_path, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image)\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "    return image_tensor\n",
        "\n",
        "def classify_connector(image_path, occ_model, ae_model, threshold, device=None):\n",
        "    \"\"\"Classifica un connettore come OK, KO o OCCLUSION.\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # STEP 1: Verifica occlusione\n",
        "    x_occ = preprocess_image(image_path, device)\n",
        "    with torch.no_grad():\n",
        "        logits = occ_model(x_occ)\n",
        "        pred_vis = torch.argmax(logits, dim=1).item()\n",
        "    \n",
        "    if pred_vis == 0:  # OCCLUSION\n",
        "        return \"OCCLUSION\", 0.0\n",
        "    \n",
        "    # STEP 2: Anomaly detection\n",
        "    x_ae = preprocess_image_for_ae(image_path, device)\n",
        "    with torch.no_grad():\n",
        "        reconstructed = ae_model(x_ae)\n",
        "        mse = nn.MSELoss(reduction='mean')\n",
        "        error = mse(reconstructed, x_ae).item()\n",
        "    \n",
        "    if error > threshold:\n",
        "        return \"KO\", error\n",
        "    else:\n",
        "        return \"OK\", error"
    ]
})

# Cell 6: Markdown - Allineamento e Cropping
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["## Funzioni di Allineamento e Cropping (da preprocess_alignment.py e crop_connectors.py)"]
})

# Cell 7: Funzioni Allineamento e Cropping
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "@dataclass\n",
        "class RelativeROI:\n",
        "    name: str\n",
        "    x_min_rel: float\n",
        "    y_min_rel: float\n",
        "    x_max_rel: float\n",
        "    y_max_rel: float\n",
        "    \n",
        "    def to_pixel_box(self, width: int, height: int, margin: int = 0):\n",
        "        x_min = int(self.x_min_rel * width)\n",
        "        y_min = int(self.y_min_rel * height)\n",
        "        x_max = int(self.x_max_rel * width)\n",
        "        y_max = int(self.y_max_rel * height)\n",
        "        x_min = max(0, x_min - margin)\n",
        "        y_min = max(0, y_min - margin)\n",
        "        x_max = min(width, x_max + margin)\n",
        "        y_max = min(height, y_max + margin)\n",
        "        return x_min, y_min, x_max, y_max\n",
        "\n",
        "def load_roi_config(roi_config_path):\n",
        "    \"\"\"Carica la configurazione ROI da JSON.\"\"\"\n",
        "    with open(roi_config_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    rois = []\n",
        "    for entry in data:\n",
        "        rois.append(RelativeROI(\n",
        "            name=entry[\"name\"],\n",
        "            x_min_rel=float(entry[\"x_min_rel\"]),\n",
        "            y_min_rel=float(entry[\"y_min_rel\"]),\n",
        "            x_max_rel=float(entry[\"x_max_rel\"]),\n",
        "            y_max_rel=float(entry[\"y_max_rel\"])\n",
        "        ))\n",
        "    return rois\n",
        "\n",
        "def detect_homography(template_gray, candidate_gray, max_features=4000, good_match_percent=0.15):\n",
        "    \"\"\"Rileva omografia tra template e candidato.\"\"\"\n",
        "    orb = cv2.ORB_create(nfeatures=max_features, fastThreshold=5, scaleFactor=1.2)\n",
        "    kp1, des1 = orb.detectAndCompute(template_gray, None)\n",
        "    kp2, des2 = orb.detectAndCompute(candidate_gray, None)\n",
        "    \n",
        "    if des1 is None or des2 is None:\n",
        "        raise RuntimeError(\"Could not extract ORB descriptors\")\n",
        "    \n",
        "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = matcher.match(des1, des2)\n",
        "    \n",
        "    if len(matches) < 15:\n",
        "        knn = matcher.knnMatch(des1, des2, k=2)\n",
        "        matches = []\n",
        "        for m, n in knn:\n",
        "            if m.distance < 0.75 * n.distance:\n",
        "                matches.append(m)\n",
        "    \n",
        "    if not matches:\n",
        "        raise RuntimeError(\"No matches found\")\n",
        "    \n",
        "    matches = sorted(matches, key=lambda m: m.distance)\n",
        "    keep = max(4, int(len(matches) * good_match_percent))\n",
        "    matches = matches[:keep]\n",
        "    \n",
        "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    \n",
        "    H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 4.0)\n",
        "    if H is None or mask is None or mask.sum() < 8:\n",
        "        raise RuntimeError(\"Homography estimation failed\")\n",
        "    return H\n",
        "\n",
        "def normalize_lighting(image_bgr):\n",
        "    \"\"\"Normalizza l'illuminazione dell'immagine.\"\"\"\n",
        "    lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
        "    l = clahe.apply(l)\n",
        "    lab = cv2.merge((l, a, b))\n",
        "    balanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Gray-world white balance\n",
        "    avg_b = np.mean(balanced[:, :, 0])\n",
        "    avg_g = np.mean(balanced[:, :, 1])\n",
        "    avg_r = np.mean(balanced[:, :, 2])\n",
        "    avg_gray = (avg_b + avg_g + avg_r) / 3.0\n",
        "    scale = np.array([avg_gray / avg_b, avg_gray / avg_g, avg_gray / avg_r])\n",
        "    wb = balanced.astype(np.float32)\n",
        "    wb *= scale\n",
        "    wb = np.clip(wb, 0, 255).astype(np.uint8)\n",
        "    return wb\n",
        "\n",
        "def align_image(image_path, reference_path=None, crop_box=None):\n",
        "    \"\"\"Allinea un'immagine a un riferimento.\"\"\"\n",
        "    image = cv2.imread(str(image_path), cv2.IMREAD_COLOR)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
        "    \n",
        "    # Se non c'√® riferimento, assume che l'immagine sia gi√† allineata\n",
        "    if reference_path is None:\n",
        "        normalized = normalize_lighting(image)\n",
        "        if crop_box:\n",
        "            xmin, ymin, xmax, ymax = crop_box\n",
        "            normalized = normalized[ymin:ymax, xmin:xmax]\n",
        "        return normalized\n",
        "    \n",
        "    # Allinea usando omografia\n",
        "    template = cv2.imread(str(reference_path), cv2.IMREAD_COLOR)\n",
        "    if template is None:\n",
        "        raise FileNotFoundError(f\"Could not read reference: {reference_path}\")\n",
        "    \n",
        "    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    H = detect_homography(template_gray, gray)\n",
        "    warped = cv2.warpPerspective(image, H, (template.shape[1], template.shape[0]))\n",
        "    \n",
        "    if crop_box:\n",
        "        xmin, ymin, xmax, ymax = crop_box\n",
        "        warped = warped[ymin:ymax, xmin:xmax]\n",
        "    \n",
        "    normalized = normalize_lighting(warped)\n",
        "    return normalized\n",
        "\n",
        "def extract_connectors(aligned_image, rois, margin=8):\n",
        "    \"\"\"Estrae i connettori da un'immagine allineata.\"\"\"\n",
        "    height, width = aligned_image.shape[:2]\n",
        "    connectors = []\n",
        "    \n",
        "    for roi in rois:\n",
        "        x_min, y_min, x_max, y_max = roi.to_pixel_box(width, height, margin=margin)\n",
        "        crop = aligned_image[y_min:y_max, x_min:x_max].copy()\n",
        "        connectors.append({\n",
        "            'name': roi.name,\n",
        "            'crop': crop,\n",
        "            'bbox': (x_min, y_min, x_max, y_max)\n",
        "        })\n",
        "    \n",
        "    return connectors"
    ]
})

# Cell 8: Markdown - Carica Immagine
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["## Carica Immagine e Processa"]
})

# Cell 9: Carica Immagine
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ‚öôÔ∏è CONFIGURAZIONE: Modifica questi path secondo le tue esigenze\n",
        "\n",
        "# Path all'immagine da analizzare\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/Project Work/Data/TOP 1/20251106110559_TOP.png\"  # ‚ö†Ô∏è MODIFICA QUESTO!\n",
        "\n",
        "# Path all'immagine di riferimento (opzionale, per allineamento)\n",
        "# Se None, assume che l'immagine sia gi√† allineata\n",
        "REFERENCE_PATH = None  # Es: \"/content/drive/MyDrive/Project Work/Data/TOP 1/20251106131917_TOP.png\"\n",
        "\n",
        "# Crop box (opzionale, formato: xmin, ymin, xmax, ymax)\n",
        "# Se None, usa l'intera immagine\n",
        "CROP_BOX = None  # Es: (302, 288, 1883, 942)\n",
        "\n",
        "# Path al file ROI config\n",
        "ROI_CONFIG_PATH = Path(\"Codice/roi_config.json\")\n",
        "\n",
        "# Carica ROI config\n",
        "rois = load_roi_config(ROI_CONFIG_PATH)\n",
        "print(f\"‚úÖ Caricate {len(rois)} ROI\")\n",
        "\n",
        "# Allinea immagine\n",
        "print(f\"\\nüì∑ Caricamento e allineamento immagine: {Path(IMAGE_PATH).name}\")\n",
        "aligned_image = align_image(IMAGE_PATH, REFERENCE_PATH, CROP_BOX)\n",
        "print(f\"   Dimensione immagine allineata: {aligned_image.shape[1]}x{aligned_image.shape[0]}\")\n",
        "\n",
        "# Estrai connettori\n",
        "print(f\"\\n‚úÇÔ∏è  Estrazione {len(rois)} connettori...\")\n",
        "connectors = extract_connectors(aligned_image, rois, margin=8)\n",
        "print(f\"‚úÖ Connettori estratti!\")"
    ]
})

# Cell 10: Markdown - Classifica e Visualizza
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["## Classifica Connettori e Visualizza Risultati"]
})

# Cell 11: Classifica Connettori
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Salva temporaneamente i crop per la classificazione\n",
        "temp_dir = Path(tempfile.mkdtemp())\n",
        "\n",
        "results = []\n",
        "for conn in connectors:\n",
        "    # Salva crop temporaneo\n",
        "    temp_path = temp_dir / f\"{conn['name']}.png\"\n",
        "    cv2.imwrite(str(temp_path), conn['crop'])\n",
        "    \n",
        "    # Classifica\n",
        "    label, error = classify_connector(str(temp_path), occ_model, ae_model, threshold, device)\n",
        "    \n",
        "    results.append({\n",
        "        'name': conn['name'],\n",
        "        'crop': conn['crop'],\n",
        "        'bbox': conn['bbox'],\n",
        "        'label': label,\n",
        "        'error': error\n",
        "    })\n",
        "\n",
        "print(\"\\nüìä Risultati classificazione:\")\n",
        "for r in results:\n",
        "    print(f\"  {r['name']}: {r['label']}\" + (f\" (error: {r['error']:.6f})\" if r['error'] > 0 else \"\"))\n",
        "\n",
        "# Colori per le label\n",
        "COLORS = {\n",
        "    'OK': (0, 255, 0),      # Verde\n",
        "    'KO': (0, 0, 255),      # Rosso\n",
        "    'OCCLUSION': (255, 165, 0)  # Arancione\n",
        "}"
    ]
})

# Cell 12: Visualizzazione
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Visualizzazione completa\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Immagine principale con bounding box\n",
        "ax1 = plt.subplot(2, 1, 1)\n",
        "img_rgb = cv2.cvtColor(aligned_image, cv2.COLOR_BGR2RGB)\n",
        "ax1.imshow(img_rgb)\n",
        "ax1.set_title(f\"PCB Image: {Path(IMAGE_PATH).name}\", fontsize=16, fontweight='bold')\n",
        "ax1.axis('off')\n",
        "\n",
        "# Disegna bounding box e label\n",
        "for r in results:\n",
        "    x_min, y_min, x_max, y_max = r['bbox']\n",
        "    color = COLORS[r['label']]\n",
        "    color_normalized = tuple(c/255.0 for c in color)\n",
        "    \n",
        "    # Rettangolo\n",
        "    rect = Rectangle((x_min, y_min), x_max-x_min, y_max-y_min, \n",
        "                     linewidth=3, edgecolor=color_normalized, facecolor='none')\n",
        "    ax1.add_patch(rect)\n",
        "    \n",
        "    # Label\n",
        "    ax1.text(x_min, y_min-5, f\"{r['name']}: {r['label']}\", \n",
        "            fontsize=10, fontweight='bold', color=color_normalized,\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "# Grid con tutti i connettori\n",
        "ax2 = plt.subplot(2, 1, 2)\n",
        "ax2.axis('off')\n",
        "ax2.set_title(\"Connettori Estratti e Classificati\", fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "# Crea grid 3x3\n",
        "for idx, r in enumerate(results):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    \n",
        "    # Subplot per ogni connettore\n",
        "    ax = plt.subplot2grid((2, 9), (1, col + row*3), fig=fig)\n",
        "    \n",
        "    crop_rgb = cv2.cvtColor(r['crop'], cv2.COLOR_BGR2RGB)\n",
        "    ax.imshow(crop_rgb)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Titolo con label colorata\n",
        "    color = COLORS[r['label']]\n",
        "    color_hex = '#%02x%02x%02x' % color\n",
        "    ax.set_title(f\"{r['name']}\\n{r['label']}\", fontsize=12, fontweight='bold', color=color_hex)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistiche\n",
        "ok_count = sum(1 for r in results if r['label'] == 'OK')\n",
        "ko_count = sum(1 for r in results if r['label'] == 'KO')\n",
        "occ_count = sum(1 for r in results if r['label'] == 'OCCLUSION')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà STATISTICHE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ OK:        {ok_count}/9\")\n",
        "print(f\"‚ùå KO:        {ko_count}/9\")\n",
        "print(f\"‚ö†Ô∏è  OCCLUSION: {occ_count}/9\")\n",
        "print(\"=\"*60)"
    ]
})

# Cell 13: Markdown - Upload Opzionale
cells.append({
    "cell_type": "markdown",
    "metadata": {},
    "source": ["## Upload Immagine Interattivo (Opzionale)"]
})

# Cell 14: Upload Immagine
cells.append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Per caricare un'immagine direttamente in Colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Prendi il primo file caricato\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    \n",
        "    # Salva in una directory temporanea\n",
        "    temp_upload_dir = Path(\"/content/temp_uploads\")\n",
        "    temp_upload_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    uploaded_path = temp_upload_dir / uploaded_filename\n",
        "    with open(uploaded_path, 'wb') as f:\n",
        "        f.write(uploaded[uploaded_filename])\n",
        "    \n",
        "    print(f\"‚úÖ Immagine caricata: {uploaded_path}\")\n",
        "    print(f\"\\n‚ö†Ô∏è  Ora modifica IMAGE_PATH nella cella sopra con:\")\n",
        "    print(f\"IMAGE_PATH = '{uploaded_path}'\")\n",
        "    print(f\"\\nPoi riesegui le celle di processing e visualizzazione.\")"
    ]
})

# Crea il notebook
notebook = {
    "cells": cells,
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

# Salva il notebook
output_path = Path("Training/step4_visual_inference.ipynb")
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print(f"‚úÖ Notebook completo generato: {output_path}")
print(f"   Totale celle: {len(cells)}")

